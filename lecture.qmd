---
title: "Data Science, AI, and ML in Systems Engineering"
subtitle: "From Classical Control to Intelligent Systems"
author: "Aykut C. Satici"
institute: "University of Texas at Dallas"
department: "Department of Systems Engineering"
date: last-modified
logo: images/logo.svg
format:
  revealjs:
    theme: [default, utd.scss]
    slide-number: true
    show-slide-number: all
    transition: fade
    center: true
    footer: "Introduction to Systems Engineering | A. C. Satici"
---

## Who am I? {.smaller}

:::: {.columns}

::: {.column width="50%"}
### My Journey
*   **Education**: BS/MS in Mechatronics (Turkey), PhD in EE & MS in Math (UTD).
*   **Research Training**: Postdoc at U. Naples (Italy) & MIT (USA).
*   **Professorate**: Assistant & Associate Professor at **Boise State University** (2017-2025).
*   **Currently**: Back home at **UT Dallas** as Associate Professor of Systems Engineering.
:::

::: {.column width="48%"}
![](images/sabanci_team.jpg){height="230" fig-align="center"}
![](images/spong-lab-650-2013-08.jpg){height="230" fig-align="center"}
:::

::::

::: notes
*   **Background**: My journey started in Mechatronics at Sabanci University in Istanbul, moving to Systems integration early on.
*   **PhD at UTD**: I focused on nonlinear control theory here at UTD, specifically geometric control of mechanical systems.
*   **International Experience**: Postdoc work in Naples (interaction control) and MIT (field robotics/DARPA Challenge) broadened my scope from theory to real-world deployment.
*   **Boise State**: Spent 9 years building the CORE Lab, focusing on Robot Locomotion and Manipulation.
*   **Return to UTD**: Excited to bring this experience back home to the Systems Engineering department, focusing on the intersection of classical control and modern AI.
:::

## BSU: Locomotion & Contact {.smaller}

:::: {.columns}

::: {.column width="45%"}
*   **The Problem**: Robots struggle with uneven terrain and physical collisions.
*   **The Solutions**:
    *   **Rimless Wheel**: Stability of passive walking.
    *   **Astria**: Drone/power-line interaction.
    *   **IWP**: Energy-shaping control for acrobatics.
:::

::: {.column width="55%"}
::: {layout-ncol=2}
{{< video videos/iwp.mp4 autoplay="true" loop="true" muted="true" >}}

{{< video videos/astria.mp4 autoplay="true" loop="true" muted="true" >}}
:::

![](images/rimless_wheel_fixed.jpg){width="50%" fig-align="center"}
:::

::::

::: notes
*   **The Core Problem**: "Contact" is the bane of classical robotics. Traditional control theory assumes smooth, continuous dynamics (differential equations).
*   **Reality**: Real-world interaction involves impacts, friction, and discrete jumps in velocity. These are "Hybrid Systems."
*   **My Solutions**:
    *   **Rimless Wheel**: We studied the passive dynamics of walking—how to be energy efficient by utilizing gravity, rather than fighting it.
    *   **Astria**: A drone that can physically interact with the environment (perching on power lines). This requires precise force control, not just position control.
    *   **IWP (Inertia Wheel Pendulum)**: A classic nonlinear control problem. We use "Energy Shaping" to swing it up and balance, managing the energy flow in the system.
:::


## Systems Engineering in Action {.smaller}

:::: {.columns}

::: {.column width="33%"}
::: {.center}
**Drones**
{{< video videos/quadrotor_lissajous.mp4 aspect-ratio=4x3 width="300" autoplay="true" loop="true" muted="true" >}}
:::
:::

::: {.column width="33%"}
::: {.center}
**Soft Juggling**
{{< video videos/soft_juggler.mp4 aspect-ratio=4x3 width="300" autoplay="true" loop="true" muted="true" >}}
:::
:::

::: {.column width="33%"}
::: {.center}
**Manipulation**
{{< video videos/pizza_tossing.mp4 aspect-ratio=4x3 width="300" autoplay="true" loop="true" muted="true" >}}
:::
:::

::::

::: {.callout-note}
Systems Engineering connects **Control Theory**, **Embedded Systems**, and **Mechanical Design** to make these robots work in the real world.
:::

::: notes
*   **Systems Integation**: These videos demonstrate what happens when you get the integration right.
    *   **Drones (Lissajous)**: This isn't just a PID controller. It involves trajectory generation, state estimation (EKF), and real-time motor mixing.
    *   **Soft Juggling**: A complex interaction problem. The paddle is soft, the ball is rigid. We need to model the deformation to predict the bounce.
    *   **Pizza Tossing**: High-speed, non-prehensile manipulation. The robot doesn't grasp the object; it manipulates it through dynamic forces (friction and inertia).
*   **The Takeaway**: Systems Engineering is the glue. It's not just code, it's not just mechanics. It's understanding how the code affects the mechanics through the electronics.
:::

## Agenda

::: columns
::: {.column width="40%"}
1.  **The Evolution**
    *   From Clockwork to Neural Nets.
2.  **The Concepts**
    *   DS vs. AI vs. ML.
3.  **The Lifecycle**
    *   Where AI fits in the V-Model.
:::

::: {.column style="width: 40%; text-align: right;"}
4.  **Live Case Study**
    *   The Furuta Pendulum.
5.  **The Future**
    *   Black boxes & Ethics.
:::
:::

# Part 1: The Evolution

## Classical vs. Intelligent Systems

::: columns
::: {.column width="50%"}
**Deterministic**
![](images/deterministic.jpg){height="300" fig-align="center"}
*Predictable. Static.*
:::

::: {.column width="50%"}
**Stochastic**
![](images/stochastic.jpg){height="300" fig-align="center"}
*Adaptive. Learning.*
:::
:::

::: notes
*   **The Paradigm Shift**:
    *   **From Clockwork**: SE traditionally dealt with *Deterministic* systems. A car engine, a bridge, a circuit. We know the equations. IF Input A, THEN Output B.
    *   **To Intelligence**: Modern systems operate in unstructured environments. The Mars Rover doesn't know the friction coefficient of the rock it's about to climb.
*   **Stochasticity**: We are moving from "Automated" (following a script) to "Autonomous" (making decisions under uncertainty).
*   **The Challenge**: How do we verify a system that isn't deterministic? That is the central question of AI in SE.
:::

## Why We Need AI

::: columns
::: {.column width="50%"}
![](https://upload.wikimedia.org/wikipedia/commons/3/32/Muxviz_GlobalRisk.png){height="400"}
:::

::: {.column width="50%"}
### The Complexity Gap
*   **Data Volume**: Terabytes/hour.
*   **Dimensionality**: Systems with 1000+ variables.
*   **Human Limits**: We cannot manually optimize these anymore.
:::
:::

::: notes
*   **Why can't we just code it?**:
    *   **Complexity**: Modern systems (e.g., global logistics, autonomous driving) have interactions that are too complex for human cognition.
    *   **Dimensionality**: You cannot write `if-then` rules for a 4K camera feed. The state space is essentially infinite.
    *   **Non-Linearity**: In complex systems, small changes can have massive effects (Chaos Theory). Analytical solutions often don't exist.
*   **The Solution**: We stop trying to write the rules. We build systems that *learn* the rules from data. We trade "Explicit Understanding" for "Performance optimization."
:::

# Part 2: The Engineer's Definition

## Disentangling the Buzzwords

![](https://upload.wikimedia.org/wikipedia/commons/1/1b/AI_hierarchy.svg){height="450" fig-align="center"}

::: notes
*   **Let's separate the hype from the engineering**:
    *   **AI (Artificial Intelligence)**: The broad umbrella. Any technique that enables computers to mimic human behavior (logic, rules, learning).
    *   **ML (Machine Learning)**: A subset of AI. Algorithms that improve through experience (data). It is statistical learning.
    *   **DL (Deep Learning)**: A subset of ML. Uses multi-layered neural networks to learn representations from raw data (e.g., pixels to objects).
    *   **Data Science**: The interdisciplinary field of extracting knowledge from data. It includes ML, but also statistics, visualization, and data engineering.
*   **Why it matters**: As systems engineers, we need to know which tool to pick. You don't need Deep Learning for a linear regression problem.
:::

## The Engineering View

::: columns
::: {.column width="50%"}
### Classical
$$ y = f(x) $$
*We know the physics (F=ma).*
*We write the equation.*
:::

::: {.column width="50%"}
### Machine Learning
$$ y \approx \hat{f}(x) $$
*We have the data points.*
*The computer finds the curve.*
:::
:::

![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/640px-Linear_regression.svg.png){height="300" fig-align="center"}

::: notes
*   **The Fundamental Difference**:
    *   **Classical Engineering**: We start with First Principles (Newton, Maxwell, Kirchhoff). We derive $y = f(x)$. This gives us guarantees but requires perfect knowledge.
    *   **Machine Learning**: We start with Data $(x, y)$. We assume a model structure $\hat{f}(x)$ (e.g., a neural net) and optimize parameters to minimize error.
*   **Universal Function Approximation**: To an engineer, a Neural Network is just a very flexible curve-fitting machine.
*   **implication**: We are replacing "Physics-based Modeling" with "Data-driven Modeling." This is powerful but dangerous—we lose the guarantee of correctness outside the training distribution.
:::

# Part 3: The Lifecycle

## The V-Model and AI

![](https://upload.wikimedia.org/wikipedia/commons/9/9b/Systems_Engineering_V_diagram.jpg){height="500" fig-align="center"}

::: notes
*   **The V-Model Crisis**:
    *   **Left Side (Design)**: How do you write requirements for a neural net? "Must detect pedestrians" is vague. The requirement becomes the *Training Set*.
    *   **Right Side (Verification)**: How do you unit test a black box? You can't trace the logic. Verification shifts to *Statistical Validation* on held-out test sets.
*   **The Dual-Track Lifecycle**: We now have two parallel tracks.
    1.  **Software Track**: Traditional Code (Rule-based, safe, auditable).
    2.  **Model Track**: Data Collection -> Training -> Evaluation.
*   **Integration**: The challenge is merging these. The AI component is just a probabilistic function call within a deterministic safety-critical system.
:::

## Operations: The Digital Twin

::: columns
::: {.column style="width: 50%; text-align: center;"}
![](https://upload.wikimedia.org/wikipedia/commons/3/38/General_Electric_GEnx_on_747-8I_prototype.jpg){height="350"}

**Physical Asset**
:::

::: {.column style="width: 50%; text-align: center;"}
![](https://upload.wikimedia.org/wikipedia/commons/7/75/Turbofan_operation.svg){height="350"}

**Digital Replica**
:::
:::

::: notes
*   **The Digital Twin Concept**:
    *   **Definition**: It is not just a 3D model. It is a probabilistic simulation that runs in parallel with the physical asset.
    *   **The Loop**:
        1.  Sensors on the engine send real-time data to the Cloud.
        2.  The Digital Twin processes this data using ML/Physics models.
        3.  The Twin predicts a bearing failure 200 hours in advance.
    *   **Value**: We move from "Reactive Maintenance" (fix it when it breaks) to "Predictive Maintenance" (fix it before it breaks).
:::

## Astria: Digital Twin {.smaller}

:::: {.columns}

::: {.column width="50%"}
*   **The Physical Asset**: Autonomous drone with a gripper.
*   **The Digital Replica**: High-fidelity physics-based simulation.
*   **Systems Goal**: Testing contact physics and control logic virtually before risking expensive hardware on power lines.
:::

::: {.column width="50%"}
::: {layout-ncol=2}
{{< video videos/astria.mp4 autoplay="true" loop="true" muted="true" >}}

{{< video videos/unclamped_then_clamped.mp4 autoplay="true" loop="true" muted="true" >}}
:::
*Real-world (Left) vs. Sim2Real Replica (Right)*
:::

::::

::: notes
*   **Safety-Critical Verification**:
    *   **The Risk**: We cannot just "try" a new AI controller on a heavy drone near high-voltage lines. A crash costs \$50k and risks starting a fire.
    *   **The Solution**: We validate in the Digital Twin. We run thousands of "Monte Carlo" simulations—varying wind, payload, and sensor noise.
    *   **Significance**: This is how SE enables AI. We build the sandbox that makes learning safe.
:::

## Rimless Wheel: Robustness {.smaller}

:::: {.columns}

::: {.column width="40%"}
*   **Systems Goal**: Walking that doesn't fall when the ground changes.
*   **Non-examples**: 
    *   Falling over (unstable).
    *   Tripping on uneven ground (fragile).
*   **Examples**:
    *   Steady rhythm on flat ground.
    *   **Robustness**: Recovering from uneven terrain.
:::

::: {.column width="60%"}
**Non-examples (Not Working)**

::: {layout-ncol=2}
{{< video videos/rw_even.mp4 autoplay="true" loop="true" muted="true" >}}

{{< video videos/rw_uneven_success.mp4 autoplay="true" loop="true" muted="true" >}}
:::

<br/>

**Examples (Working Well)**

::: {layout-ncol=2}
{{< video videos/rw_failing.mp4 autoplay="true" loop="true" muted="true" >}}

{{< video videos/rw_uneven.mp4 autoplay="true" loop="true" muted="true" >}}
:::
:::

::::

::: notes
*   **Robustness in SE**: In classical control, we check feedback stability (Eigenvalues).
*   **The Problem with Contact**: When a foot hits the ground, the physics change instantly (impact). Eigenvalues don't work well here.
*   **Basin of Attraction**: Instead, we analyze the "Basin of Attraction"—how big of a disturbance (slope change, push) can the system recover from?
*   **Visual**: The videos show "stable limit cycles"—a repeating loop of states that handles disturbances naturally without explicit reprogramming.
:::

# Part 4: Live Case Study {background-color="#154734"}

## How AI Learns to Control

::: columns
::: {.column width="60%"}
![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/640px-Reinforcement_learning_diagram.svg.png){height="350"}
:::

::: {.column width="40%"}
**Reinforcement Learning**

1.  **Observe** state.
2.  **Take Action** (Voltage).
3.  **Get Reward** (+1 if upright).
:::
:::

::: notes
*   **Reinforcement Learning (RL)**:
    *   **The Agent**: The AI controller.
    *   **The Environment**: The physical laws (gravity, friction).
    *   **The Loop**:
        1.  **Observe**: "I am falling left."
        2.  **Act**: "Spin motor right."
        3.  **Reward**: "I stayed up for 0.1s! (+1 point)."
    *   **The Magic**: We don't tell it F=ma. It *discovers* F=ma by trying to maximize points.
:::

## Sim2Real: The Reality Check

::: columns
::: {.column style="width: 50%; text-align: center;"}
**Simulation**

![](https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg){height="300"}

*Perfect Math*
:::

::: {.column style="width: 50%; text-align: center;"}
**Reality**

![](https://upload.wikimedia.org/wikipedia/commons/f/f0/RotaryInvertedPendulum.JPG){height="300" style="filter: blur(1px) contrast(120%);"}

*Noise & Friction*
:::
:::

::: notes
*   **The Reality Gap**:
    *   **Simulation**: Perfect friction, no delay, perfect sensors.
    *   **Reality**: Grease, 10ms delay, noisy encoders.
    *   **The Failure Mode**: A policy trained only in a perfect sim will fail instantly in reality. It "overfits" to the physics engine.
    *   **Domain Randomization**: We train the AI in a "Shiftng Simulation"—randomly changing gravity, mass, and friction every episode. This forces the AI to learn a *robust* policy that works across a range of physics.
:::

## Learning to Walk: RL in Action {.smaller}

:::: {.columns}

::: {.column width="40%"}
*   **The Goal**: Make the robot walk forward.
*   **The Process**:
    *   Start with zero knowledge.
    *   Try random actions.
    *   Get rewarded for forward motion.
    *   **Result**: Emergent walking behavior.
:::

::: {.column width="60%"}
{{< video videos/walking_RL.mp4 autoplay="true" loop="true" muted="true" >}}
:::

::::

::: notes
*   **Emergent Behavior**: We defined the *goal* (move +z), not the *method*.
*   **The Result**: The AI discovered a "galloping" gait. This is often more efficient than what a human would code manually.
*   **Caveat**: It also discovered that vibrating on the spot earned points. We had to fix the reward function. This is "Reward Hacking" — a major safety concern in AI.
:::


## The Furuta Pendulum

::: columns
::: {.column width="50%"}
*   **Goal**: Balance upright.
*   **Actuator**: Base Motor.
*   **Sensor**: Encoders.
:::

::: {.column width="50%"}
<!-- Use your local image here if available, otherwise this wiki one works -->
![](images/furuta.jpg){height="400"}
:::
:::

::: notes
*   **The Hardware**:
    *   **Underactuated**: 2 degrees of freedom (arm, pendulum), but only 1 motor. You can't just "command" the pendulum to go up. You have to swing it.
    *   **Nonlinear**: Gravity acts differently depending on the angle.
    *   **The Challenge**: Using a Neural Network to swing this up and balance it, replacing the 3 pages of differential equations I usually use.
:::

## Integration: The Furuta Pendulum {.smaller}

:::: {.columns}

::: {.column width="50%"}
::: {.center}
**Controller Architecture**
![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/640px-Artificial_neural_network.svg.png){height="400" fig-align="center"}
:::
:::

::: {.column width="50%"}
::: {.center}
**Performance (Pre-Move)**
{{< video videos/furuta.mp4 height="400" autoplay="true" loop="true" muted="true" >}}
:::
:::

::::

*   **Hardware**: Assembled (Physical).
*   **Controller**: Neural Network (Code).
*   **Current Status**: <span style="color:orange">**Broken (Moved to Dallas).**</span>
    *   *Video shows successful operation from Boise State lab.*

::: notes
*   **The Integration Plan**:
    1.  **Phase 1 (Sim)**: Train the Policy using PPO (Proximal Policy Optimization).
    2.  **Phase 2 (Gap verify)**: Test the policy with added noise.
    3.  **Phase 3 (Deploy)**: Run the neural net on the embedded microcontroller (Raspberry Pi / Arduino).
*   **Status**: The hardware is physically here. The code is trained. We are working on the *Interface*—getting the Python weights to run on the C++ driver.
:::


# Part 5: The Future

## The Black Box Problem

::: columns
::: {.column width="50%"}
### Classical Code
`IF speed > 50 THEN brake`
*(Traceable)*
:::

::: {.column width="50%"}
### Neural Network
`0.23 * x1 + 0.99 * x2 ...`
*(Opaque)*
:::
:::

![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/640px-Colored_neural_network.svg.png){height="300" fig-align="center"}

::: notes
*   **Traceability**: In SE, we need to trace every line of code to a requirement.
*   **Explainable AI (XAI)**:
    *   **Saliency Maps**: "Why did the car stop? Because it saw these pixels."
    *   **Counterfactuals**: "What if the pedestrian was wearing blue?"
*   **The Trade-off**: We often trade *Accuracy* (Deep Learning) for *Interpretability* (Decision Trees). For safety-critical systems, we might prefer a dumber but understandable model.
:::

## The Ethics of AI

![](https://upload.wikimedia.org/wikipedia/commons/f/fd/Trolley_Problem.svg){height="400" fig-align="center"}

::: notes
*   **The Trolley Problem**: It's not just philosophy anymore. It's code.
*   **Accountability**: If the AI makes a mistake, who is liable? The engineer? The company? The dataset creator?
*   **Bias**: If the training data only has sunny days, the car will kill people in the rain. Systems Engineering must ensure *Data Coverage* just like we ensure *Test Coverage*.
:::

# Conclusion

## Summary

::: columns
::: {.column width="30%"}
![](https://upload.wikimedia.org/wikipedia/commons/1/18/Gears-solid.svg){height="200"}
:::

::: {.column width="70%"}
1.  **Systems are changing**: From static to dynamic.
2.  **Tools**: Python/MATLAB + Control Theory.
3.  **Mindset**: AI is just a subsystem. Integration is key.
:::
:::

## Questions?

::: {style="text-align: center"}
**Thank you!**

*Aykut C. Satici*

*Department of Systems Engineering* \
*University of Texas at Dallas*

*Questions?*
:::